{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sitting-dress",
   "metadata": {},
   "source": [
    "# Importing Libraries and Supporting python files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "marine-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "import copy\n",
    "import os\n",
    "import numpy as np\n",
    "from options import Parameters\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "different-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling  parameters class from Arguments file\n",
    "args = Parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-tribe",
   "metadata": {},
   "source": [
    "# Embedding Network\n",
    "#### The embedding sub-network  consists of a deep convolutional network for feature extraction and a  nonparametric one-shot classifier\n",
    "# CNet\n",
    "1. Given an input image I, we\n",
    "use a residual network  to produce its feature representation fθemb (I)\n",
    "2. a fully-connected\n",
    "layer on top of the embedding sub-network with a crossentropy loss (CELoss), that outputs |Cbase| scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "romantic-ending",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "developmental-plaza",
   "metadata": {},
   "outputs": [],
   "source": [
    "class resnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(resnet,self).__init__()\n",
    "       \n",
    "        '''\n",
    "        Update the resnet model last layer to match the weights\n",
    "        '''\n",
    "        resnet18 = torchvision.models.resnet18(pretrained=False)\n",
    "        num_features=resnet18.fc.in_features\n",
    "        resnet18.fc=nn.Linear(num_features,64)\n",
    "        '''\n",
    "        Updated the resnet model according to the weights(.t7)\n",
    "        '''\n",
    "\n",
    "        state_dict = torch.load(r'softRandom.t7',map_location=torch.device('cpu'))\n",
    "\n",
    "        names=[]\n",
    "        for k , v in resnet18.state_dict().items():\n",
    "            names.append(k)\n",
    "        i=0\n",
    "\n",
    "        new_state_dict = OrderedDict()\n",
    "\n",
    "        for k, v in state_dict.items():\n",
    "            new_state_dict[names[i]] = v\n",
    "            i=i+1\n",
    "        resnet18.load_state_dict(new_state_dict)\n",
    "        \n",
    "        '''\n",
    "        Store the last layer weights before removing\n",
    "        '''\n",
    "        self.fc_layer_weight=resnet18.state_dict()[names[-2]]\n",
    "#         resnet_updated.fc=Identity()\n",
    "        \n",
    "        '''\n",
    "        Create a sequence of layers\n",
    "        '''\n",
    "        self.conv1=resnet18.conv1\n",
    "        self.conv1.load_state_dict(resnet18.conv1.state_dict())\n",
    "        \n",
    "        self.bn1=resnet18.bn1\n",
    "        self.bn1.load_state_dict(resnet18.bn1.state_dict())\n",
    "        \n",
    "        self.relu=resnet18.relu\n",
    "        self.maxpool=resnet18.maxpool\n",
    "        \n",
    "        \n",
    "        self.layer1=resnet18.layer1\n",
    "        self.layer1.load_state_dict(resnet18.layer1.state_dict())\n",
    "        self.layer2=resnet18.layer2\n",
    "        self.layer2.load_state_dict(resnet18.layer2.state_dict())\n",
    "        \n",
    "        self.layer3=resnet18.layer3\n",
    "        self.layer3.load_state_dict(resnet18.layer3.state_dict())\n",
    "        self.layer4=resnet18.layer4\n",
    "        self.layer4.load_state_dict(resnet18.layer4.state_dict())\n",
    "        self.avgpool=resnet18.avgpool\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.bn1(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.maxpool(x)\n",
    "        layer1 = self.layer1(x) # (, 64L, 56L, 56L)\n",
    "        layer2 = self.layer2(layer1) # (, 128L, 28L, 28L)\n",
    "        layer3 = self.layer3(layer2) # (, 256L, 14L, 14L)\n",
    "        layer4 = self.layer4(layer3) # (,512,7,7)\n",
    "        x = self.avgpool(layer4) # (,512,1,1)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-dominican",
   "metadata": {},
   "source": [
    "# Deformation Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "mounted-planner",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "spoken-stranger",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeformationNetwork(nn.Module):\n",
    "    '''\n",
    "        Two branch's performance are similar one branch's\n",
    "        So we use one branch here\n",
    "        Deeper attention network do not bring in benifits\n",
    "        So we use small network here\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(DeformationNetwork,self).__init__()\n",
    "        def conv(inp,out):\n",
    "            return nn.Sequential(nn.Conv2d(inp,out,3,padding=1),\n",
    "                           nn.BatchNorm2d(out),\n",
    "                           nn.ReLU(),\n",
    "                           nn.MaxPool2d(2)\n",
    "                          )\n",
    "        self.encoder=nn.Sequential(conv(6,32), #'6*224*224'\n",
    "                   \n",
    "                                conv(32,64),#'6*224*224'\n",
    "                                conv(64,64),#'6*224*224'\n",
    "                                conv(64,32),#'6*224*224'\n",
    "                                conv(32,16),\n",
    "                                Flatten() )\n",
    "    def forward(self,x):\n",
    "        \"\"\"                 \n",
    "    inputs: Batchsize*3*224*224\n",
    "    outputs: Batchsize*100\n",
    "    \"\"\"\n",
    "        outputs=self.encoder(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "institutional-expansion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 784]                 --\n",
      "|    └─Sequential: 2-1                   [-1, 32, 112, 112]        --\n",
      "|    |    └─Conv2d: 3-1                  [-1, 32, 224, 224]        1,760\n",
      "|    |    └─BatchNorm2d: 3-2             [-1, 32, 224, 224]        64\n",
      "|    |    └─ReLU: 3-3                    [-1, 32, 224, 224]        --\n",
      "|    |    └─MaxPool2d: 3-4               [-1, 32, 112, 112]        --\n",
      "|    └─Sequential: 2-2                   [-1, 64, 56, 56]          --\n",
      "|    |    └─Conv2d: 3-5                  [-1, 64, 112, 112]        18,496\n",
      "|    |    └─BatchNorm2d: 3-6             [-1, 64, 112, 112]        128\n",
      "|    |    └─ReLU: 3-7                    [-1, 64, 112, 112]        --\n",
      "|    |    └─MaxPool2d: 3-8               [-1, 64, 56, 56]          --\n",
      "|    └─Sequential: 2-3                   [-1, 64, 28, 28]          --\n",
      "|    |    └─Conv2d: 3-9                  [-1, 64, 56, 56]          36,928\n",
      "|    |    └─BatchNorm2d: 3-10            [-1, 64, 56, 56]          128\n",
      "|    |    └─ReLU: 3-11                   [-1, 64, 56, 56]          --\n",
      "|    |    └─MaxPool2d: 3-12              [-1, 64, 28, 28]          --\n",
      "|    └─Sequential: 2-4                   [-1, 32, 14, 14]          --\n",
      "|    |    └─Conv2d: 3-13                 [-1, 32, 28, 28]          18,464\n",
      "|    |    └─BatchNorm2d: 3-14            [-1, 32, 28, 28]          64\n",
      "|    |    └─ReLU: 3-15                   [-1, 32, 28, 28]          --\n",
      "|    |    └─MaxPool2d: 3-16              [-1, 32, 14, 14]          --\n",
      "|    └─Sequential: 2-5                   [-1, 16, 7, 7]            --\n",
      "|    |    └─Conv2d: 3-17                 [-1, 16, 14, 14]          4,624\n",
      "|    |    └─BatchNorm2d: 3-18            [-1, 16, 14, 14]          32\n",
      "|    |    └─ReLU: 3-19                   [-1, 16, 14, 14]          --\n",
      "|    |    └─MaxPool2d: 3-20              [-1, 16, 7, 7]            --\n",
      "|    └─Flatten: 2-6                      [-1, 784]                 --\n",
      "==========================================================================================\n",
      "Total params: 80,688\n",
      "Trainable params: 80,688\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 449.04\n",
      "==========================================================================================\n",
      "Input size (MB): 1.15\n",
      "Forward/backward pass size (MB): 40.24\n",
      "Params size (MB): 0.31\n",
      "Estimated Total Size (MB): 41.70\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Sequential: 1-1                        [-1, 784]                 --\n",
       "|    └─Sequential: 2-1                   [-1, 32, 112, 112]        --\n",
       "|    |    └─Conv2d: 3-1                  [-1, 32, 224, 224]        1,760\n",
       "|    |    └─BatchNorm2d: 3-2             [-1, 32, 224, 224]        64\n",
       "|    |    └─ReLU: 3-3                    [-1, 32, 224, 224]        --\n",
       "|    |    └─MaxPool2d: 3-4               [-1, 32, 112, 112]        --\n",
       "|    └─Sequential: 2-2                   [-1, 64, 56, 56]          --\n",
       "|    |    └─Conv2d: 3-5                  [-1, 64, 112, 112]        18,496\n",
       "|    |    └─BatchNorm2d: 3-6             [-1, 64, 112, 112]        128\n",
       "|    |    └─ReLU: 3-7                    [-1, 64, 112, 112]        --\n",
       "|    |    └─MaxPool2d: 3-8               [-1, 64, 56, 56]          --\n",
       "|    └─Sequential: 2-3                   [-1, 64, 28, 28]          --\n",
       "|    |    └─Conv2d: 3-9                  [-1, 64, 56, 56]          36,928\n",
       "|    |    └─BatchNorm2d: 3-10            [-1, 64, 56, 56]          128\n",
       "|    |    └─ReLU: 3-11                   [-1, 64, 56, 56]          --\n",
       "|    |    └─MaxPool2d: 3-12              [-1, 64, 28, 28]          --\n",
       "|    └─Sequential: 2-4                   [-1, 32, 14, 14]          --\n",
       "|    |    └─Conv2d: 3-13                 [-1, 32, 28, 28]          18,464\n",
       "|    |    └─BatchNorm2d: 3-14            [-1, 32, 28, 28]          64\n",
       "|    |    └─ReLU: 3-15                   [-1, 32, 28, 28]          --\n",
       "|    |    └─MaxPool2d: 3-16              [-1, 32, 14, 14]          --\n",
       "|    └─Sequential: 2-5                   [-1, 16, 7, 7]            --\n",
       "|    |    └─Conv2d: 3-17                 [-1, 16, 14, 14]          4,624\n",
       "|    |    └─BatchNorm2d: 3-18            [-1, 16, 14, 14]          32\n",
       "|    |    └─ReLU: 3-19                   [-1, 16, 14, 14]          --\n",
       "|    |    └─MaxPool2d: 3-20              [-1, 16, 7, 7]            --\n",
       "|    └─Flatten: 2-6                      [-1, 784]                 --\n",
       "==========================================================================================\n",
       "Total params: 80,688\n",
       "Trainable params: 80,688\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 449.04\n",
       "==========================================================================================\n",
       "Input size (MB): 1.15\n",
       "Forward/backward pass size (MB): 40.24\n",
       "Params size (MB): 0.31\n",
       "Estimated Total Size (MB): 41.70\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "deform=DeformationNetwork()\n",
    "summary(deform,(6,224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-keeping",
   "metadata": {},
   "source": [
    "# IDEMENET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "infrared-bidding",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IDeMeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IDeMeNet,self).__init__()\n",
    "        \n",
    "        self.deform=DeformationNetwork()\n",
    "        \n",
    "        self.embedding=resnet()\n",
    "        \n",
    "        #patch weight for Linear combination of probe and gallery images\n",
    "        #defualt patch size if 3*3\n",
    "        self.patch=nn.Sequential(nn.Linear(784,3*3))\n",
    "        \n",
    "        '''\n",
    "        FC layer in Embedding Network\n",
    "        use the weight we stored in to a separate variable before making in \n",
    "        to identity (in other words remove)\n",
    "        '''\n",
    "        self.fc=nn.Linear(512,64)\n",
    "        self.fc.weight=torch.nn.parameter.Parameter(self.embedding.fc_layer_weight)\n",
    "        \n",
    "        \n",
    "    def forward(self,probe,gallery=1,syn_embedding=None,fixSquare=1,oneSquare=1,mode=None):\n",
    "        if mode=='deform_embedding':\n",
    "            batch_size=probe.size(0)\n",
    "            feature=self.deform(torch.cat((probe,gallery),1))\n",
    "            weight=self.patch(feature)\n",
    "            '''\n",
    "            Reshape weights to perform linear operation\n",
    "            '''\n",
    "            patch_weight=weight.view(batch_size,3*3,1,1,1)\n",
    "            patch_weight=patch_weight.expand(batch_size,3*3,3,224,224)\n",
    "            patch_weight=patch_weight*fixSquare #[batch,9,3,224,224]\n",
    "            patch_weight=torch.sum(patch_weight,dim=1) #[batch 3,224,224]\n",
    "\n",
    "            img_syn=patch_weight*probe+(oneSquare-patch_weight)*gallery\n",
    "            syn_embedding=self.embedding(img_syn)\n",
    "            return syn_embedding,weight,feature\n",
    "        elif mode=='fully_connected':\n",
    "            fc_output=self.fc(syn_embedding)\n",
    "            return fc_output\n",
    "        elif mode=='feature_extraction':\n",
    "            \n",
    "            feature=self.embedding(probe)\n",
    "            return feature\n",
    "        \n",
    "IDeMeNet=IDeMeNet()      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"N.o GPU's using \",torch.cuda.device_count())\n",
    "# IDeMeNet=nn.DataParallel(IDeMeNet)\n",
    "# # IDeMeNet=IDeMeNet.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-invalid",
   "metadata": {},
   "source": [
    "# Set the optimization parameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert args.train_from_scratch==True\n",
    "    \n",
    "\n",
    "optimizer_deform=torch.optim.Adam([\n",
    "                {'params':IDeMeNet.deform.parameters()},\n",
    "                {'params':IDeMeNet.patch.parameters(),'lr':args.LR}\n",
    "                ],lr=args.LR,eps=1e-04)\n",
    "optimizer_classifer=torch.optim.Adam([\n",
    "                {'params':IDeMeNet.embedding.parameters(),'lr':args.LR*0.1},\n",
    "                {'params':IDeMeNet.fc.parameters(),'lr':args.LR}\n",
    "                ],lr=args.LR*0.2,eps=1e-05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-kansas",
   "metadata": {},
   "source": [
    "# The paper suggests to uses learing rate scheduler which led to better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-regard",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "deform_lr_scheduler=lr_scheduler.StepLR(optimizer_deform,step_size=40,gamma=0.5)\n",
    "embedding_lr_scheduler=lr_scheduler.StepLR(optimizer_classifer,step_size=40,gamma=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-approach",
   "metadata": {},
   "source": [
    "# Load the train,test and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loading import OneShot_Imagenet\n",
    "\n",
    "def worker_init_fn(worker_id):                                                         \n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "\n",
    "image_datasets={x:OneShot_Imagenet(path=r'C:\\Users\\vsankepa\\Desktop\\Untitled Folder 1',type_=x,ways=args.trainways if x=='train' else args.ways,\n",
    "                                  shots=args.shots,test_num=args.test_num,epoch=args.epoch,gallery_img=args.gallery_img) for x in ['train','test','val']}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-special",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders={x:torch.utils.data.DataLoader(image_datasets[x],\n",
    "                                          batch_size=1,shuffle=True if x=='train' else False\n",
    "#                                           ,num_workers=n_threads,   commented for GPU\n",
    "#                                            worker_init_fn=worker_init_fn\n",
    "                                          ) for x in ['train','test','val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Gallery Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-incentive",
   "metadata": {},
   "outputs": [],
   "source": [
    "gallery=image_datasets['test'].gallery # train or test does not matter it will give the same data check the function\n",
    "gallery_feature=image_datasets['test'].get_features(IDeMeNet,args.batch_size) #torch.Size([1920, 512])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-romance",
   "metadata": {},
   "source": [
    "# Supporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(model,probe_images,requires_grad):\n",
    "    batch=(len(probe_images)+args.batch_size-1)//args.batch_size\n",
    "    for i in range(batch):\n",
    "        features=model(Variable(probe_images[i*args.batch_size:(i+1)*args.batch_size],requires_grad=requires_grad),mode='feature_extraction')\n",
    "        \n",
    "        if i==0:\n",
    "#             print(i)\n",
    "            all_features=features\n",
    "#             print(all_features.shape)\n",
    "        else:\n",
    "            all_features=torch.cat((all_features,features),dim=0)\n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-defeat",
   "metadata": {},
   "source": [
    "# Perform Linear operation of Embeddings of support and gallery images\n",
    "\n",
    "## Creating a weight matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Weight matrix pre-process\n",
    "patch_xl = []\n",
    "patch_xr = []\n",
    "patch_yl = []\n",
    "patch_yr = []\n",
    "\n",
    "if args.patch_size == 3:\n",
    "    point = [0,74,148,224]\n",
    "elif args.patch_size == 5:\n",
    "    point = [0,44,88,132,176,224]\n",
    "elif args.patch_size == 7:\n",
    "    point = [0,32,64,96,128,160,192,224]\n",
    "\n",
    "for i in range(args.patch_size):\n",
    "    for j in range(args.patch_size):\n",
    "        patch_xl.append(point[i])\n",
    "        patch_xr.append(point[i+1])\n",
    "        patch_yl.append(point[j])\n",
    "        patch_yr.append(point[j+1])\n",
    "\n",
    "fixSquare = torch.zeros(1,args.patch_size*args.patch_size,3,224,224).float()\n",
    "for i in range(args.patch_size*args.patch_size):\n",
    "    fixSquare[:,i,:,patch_xl[i]:patch_xr[i],patch_yl[i]:patch_yr[i]] = 1.00\n",
    "fixSquare = fixSquare  #.cuda()\n",
    "\n",
    "oneSquare = torch.ones(1,3,224,224).float()\n",
    "oneSquare = oneSquare  #.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-ordinance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(x, y):\n",
    "    # x: N x D \n",
    "    # y: M x D \n",
    "    n = x.size(0) #192\n",
    "    m = y.size(0) # 5\n",
    "    d = x.size(1) #512\n",
    "#     assert d == y.size(1)\n",
    "    x = x.unsqueeze(1).expand(n, m, d) # [192,5,512]\n",
    "    y = y.unsqueeze(0).expand(n, m, d) #[192,5,512]\n",
    "    # To accelerate training, but observe little effect\n",
    "\n",
    "    return (torch.pow(x - y, 2)).sum(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-ability",
   "metadata": {},
   "source": [
    "# As we have less data we are augmenting dataset based on the euclidean distance between classes for support dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-dominant",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " probe Image is same as support Image .Probe Image before augmenting dataset based on distance and Support Image after augmenting\n",
    " the dataset\n",
    "'''\n",
    "\n",
    "def aug_images_basedOnDistance(support_images,support_features,support_group,support_class,ways):\n",
    "    '''\n",
    "    Calculate the distance between the support/probe features\n",
    "    and gallery features\n",
    "    IMP: this step will separate gallery and probe/support images based on the distance\n",
    "    with out this step gallery and probe/support images are same\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    support_center=support_features.view(ways,args.shots,-1).mean(dim=1) # [5,5,512] --> [5,512]\n",
    "    batch_size=len(gallery_feature)//10 #1920//10=192\n",
    "    dists=euclidean_dist(gallery_feature[:batch_size],support_center)\n",
    "    with torch.no_grad():\n",
    "        for i in range(1,10): #adding one will include\n",
    "            \n",
    "            distances=euclidean_dist(gallery_feature[i*batch_size:(i+1)*batch_size],support_center)\n",
    "            \n",
    "            dists=torch.cat((dists,distances),dim=0)\n",
    "    dists=dists.transpose(0,1) ## [ways,ways*Gallery_size] check self.train_data in loadData\n",
    "    probe_images=torch.FloatTensor(ways*args.shots*(1+args.augnum),3,224,224) # [5,5,6,3,224,224]\n",
    "    gallery_images=torch.FloatTensor(ways*args.shots*(1+args.augnum),3,224,224) # [5,5,6,3,224,224]\n",
    "    probe_group=torch.FloatTensor(ways*args.shots*(1+args.augnum),1)#way number  # [5,5,6,1]\n",
    "    probe_class=torch.FloatTensor(ways*args.shots*(1+args.augnum),1) # class # [5,5,6,1]\n",
    "    _,distance_ind=torch.topk(dists,args.chooseNum,dim=1,largest=False)  #returns top chooseNum distances in ascending order.\n",
    "                                                #I think he are we are using gallery images which are close to original images in euclidean distance\n",
    "    for i in range(ways):\n",
    "        for j in range(args.shots):\n",
    "            probe_images[i*args.shots*(1+args.augnum)+j*(1+args.augnum)+0]=support_images[i*args.shots+j]\n",
    "            probe_group[i*args.shots*(1+args.augnum)+j*(1+args.augnum)+0]=support_group[i*args.shots+j]\n",
    "            probe_class[i*args.shots*(1+args.augnum)+j*(1+args.augnum)+0]=support_class[i*args.shots+j]\n",
    "            \n",
    "            gallery_images[i*args.shots*(1+args.augnum)+j*(1+args.augnum)+0]=support_images[i*args.shots+j]\n",
    "            \n",
    "            for k in range(args.augnum):\n",
    "                p=np.random.randint(0,2)\n",
    "                if p==0:                                                   #1+k is because k starts from 0 and oth position has an image already\n",
    "                    probe_images[i*args.shots*(1+args.augnum)+j*(1+args.augnum)+1+k]=torch.flip(probe_images[i*args.shots+j],[2])\n",
    "                else:\n",
    "                    probe_images[i*args.shots*(1+args.augnum)+j*(1+args.augnum)+1+k]=probe_images[i*args.shots+j]\n",
    "                    \n",
    "                probe_group[i*args.shots*(1+args.augnum)+j*(1+args.augnum)+1+k]=support_group[i*args.shots+j]\n",
    "                probe_class[i*args.shots*(1+args.augnum)+j*(1+args.augnum)+1+k]=support_class[i*args.shots+j]\n",
    "                \n",
    "                \n",
    "                \n",
    "                choose=np.random.randint(0,args.chooseNum)                                       # train or test does not matter it will select all train images for gallery\n",
    "                gallery_images[i*args.shots*(1+args.augnum)+j*(1+args.augnum)+1+k]=image_datasets['test'].get_gallery_images(gallery[distance_ind[i][choose]])\n",
    "                \n",
    "    \n",
    "    return probe_images,gallery_images,probe_group,probe_class     \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-china",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-muscle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,num_epochs=25):\n",
    "    summary=dict()\n",
    "    num_epochs=20\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float('inf')\n",
    "    emb_loss=nn.CrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Running Epoch -->',epoch)\n",
    "\n",
    "\n",
    "        for phase in ['train','test']:\n",
    "            if phase=='train':\n",
    "                deform_lr_scheduler.step()\n",
    "                embedding_lr_scheduler.step()\n",
    "\n",
    "            model.train(False)\n",
    "            loss,acc=0.0,0\n",
    "            classifier_loss,classifier_acc=0,0\n",
    "\n",
    "            weights={}\n",
    "            for k in range(args.patch_size*args.patch_size):\n",
    "                weights[str(k)]=[]\n",
    "\n",
    "            count=0\n",
    "            for i , (support_images,support_group,support_class,test_images,test_group,test_class) in enumerate(dataloaders['train']):\n",
    "\n",
    "                '''\n",
    "                x = torch.tensor([1, 2, 3, 4]) # (4)\n",
    "                x.unsqueeze(1).shape #(4,1)\n",
    "                x=x.unsqueeze(0).shape #(1,4)\n",
    "\n",
    "                x.squueze(0).shape # (4)\n",
    "\n",
    "                '''\n",
    "                count=count+1\n",
    "                support_images=support_images.squeeze(0) #torch.Size([25, 3, 224, 224])\n",
    "                support_group=support_group.squeeze(0) #torch.Size([25, 1])\n",
    "                support_class=support_class.squeeze(0) #torch.Size([25, 1])\n",
    "\n",
    "                test_images=test_images.squeeze(0) #torch.Size([25, 3, 224, 224])\n",
    "                test_class=test_class.squeeze(0) #torch.Size([25, 3, 224, 224])\n",
    "                ways=int(support_images.size(0)//args.shots)             \n",
    "                support_features=extract_feature(model,support_images,requires_grad=True) #torch.Size([25, 512])\n",
    "                test_features=extract_feature(model,test_images,requires_grad=True) #torch.Size([25, 512])\n",
    "                probe_images,gallery_images,probe_group,probe_class=aug_images_basedOnDistance(support_images,support_features,support_group,support_class,ways=ways)\n",
    "                batch=len(probe_images+args.batch_size-1)//args.batch_size\n",
    "                first=True\n",
    "                for b in range(batch):\n",
    "                    if b==batch-1:\n",
    "                        remaining=probe_images.size(0)-b*args.batch_size\n",
    "                        syn_embedding,patch_weight,features=model(Variable(probe_images[b*args.batch_size:],requires_grad=True),\n",
    "                            Variable(gallery_images[b*args.batch_size:],requires_grad=True),\n",
    "                            fixSquare=Variable(fixSquare.expand(remaining,args.patch_size*args.patch_size,3,224,224),requires_grad=False),\n",
    "                            oneSquare=Variable(oneSquare.expand(remaining,3,224,224),requires_grad=False),\n",
    "                            mode='deform_embedding'\n",
    "                            )\n",
    "                        _cls = model(None,syn_embedding=syn_embedding,gallery=None,fixSquare=1,oneSquare=1,mode='fully_connected')\n",
    "\n",
    "                    else:\n",
    "                        syn_embedding,patch_weight,features=model(Variable(probe_images[b*args.batch_size:(b+1)*args.batch_size],requires_grad=True),\n",
    "                            Variable(gallery_images[b*args.batch_size:(b+1)*args.batch_size],requires_grad=True),\n",
    "                            fixSquare=Variable(fixSquare.expand(args.batch_size,args.patch_size*args.patch_size,3,224,224),requires_grad=False),\n",
    "                            oneSquare=Variable(oneSquare.expand(args.batch_size,3,224,224),requires_grad=False),\n",
    "                            mode='deform_embedding'\n",
    "                            )\n",
    "                        _cls = model(None,syn_embedding=syn_embedding,gallery=None,fixSquare=1,oneSquare=1,mode='fully_connected')\n",
    "\n",
    "                    if b==0:\n",
    "                        all_syn_embedding=syn_embedding\n",
    "                        all_patch_weight=patch_weight\n",
    "                        all_features=features\n",
    "                        all_classes=_cls\n",
    "                    else:\n",
    "                        all_syn_embedding=torch.cat((all_syn_embedding,syn_embedding),dim=0)\n",
    "                        all_patch_weight=torch.cat((all_patch_weight,patch_weight),dim=0)\n",
    "                        all_features=torch.cat((all_features,features),dim=0)\n",
    "                        all_classes=torch.cat((all_classes,_cls),dim=0)\n",
    "                all_patch_weight=all_patch_weight.transpose(1,0)  \n",
    "                for k in range(args.patch_size*args.patch_size):\n",
    "                    weights[str(k)]=weights[str(k)]+all_patch_weight[k].reshape(-1).tolist()\n",
    "                syn_embedding_mean=all_syn_embedding.view(ways,args.shots*(1+args.augnum),-1).mean(1) #[ways,512]\n",
    "                dists=euclidean_dist(test_features,syn_embedding_mean) #[ways*test_num,ways]\n",
    "\n",
    "                log_prob=F.log_softmax(-dists,dim=1).view(ways,args.test_num,-1)  # [ways,test_num,ways]]\n",
    "\n",
    "                loss_val=-log_prob.gather(2,test_group.view(ways,args.test_num,1)).view(-1).mean()\n",
    "\n",
    "                val,ind=log_prob.max(2) # 0- columns , 1-rows ,2- shape dim-0*dim-1\n",
    "                acc_val=torch.eq(ind,test_group.view(ways,args.test_num)).float().mean()\n",
    "\n",
    "\n",
    "                loss+=loss_val.item()\n",
    "                acc+=acc_val.item()\n",
    "\n",
    "            #back propagation in training phase\n",
    "                if phase=='train':\n",
    "                    if args.fix_deform==0:\n",
    "                        optimizer_deform.zero_grad()\n",
    "                        loss_val.backward(retain_graph=True)\n",
    "                        optimizer_deform.step()\n",
    "                    ind,pred=torch.max(all_classes,1)\n",
    "                    probe_class=probe_class.view(probe_class.size(0))\n",
    "\n",
    "                    entropy_loss=emb_loss(all_classes,probe_class.long())\n",
    "                    if epoch!=0 and args.fix_emb==True:\n",
    "                        optimizer_classifer.zero_grad()\n",
    "                        entropy_loss.backward()\n",
    "                        optimizer_classifer.step()\n",
    "\n",
    "                    classifier_loss+=entropy_loss.item()\n",
    "                    classifier_acc+=torch.eq(pred,probe_class.view(-1)).float().mean()\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            epoch_loss=classifier_loss/float(count)\n",
    "            epoch_acc=classifier_acc/float(count)\n",
    "\n",
    "            epoch_classifier_loss=classifier_loss/float(count)\n",
    "            epoch_classifier_acc=classifier_acc/float(count)\n",
    "\n",
    "\n",
    "            summary[str(epoch)+'-'+phase]={\n",
    "                phase+'loss': epoch_loss,\n",
    "                phase+'accuracy': epoch_acc,\n",
    "                phase+'_classifier_loss': epoch_classifier_loss,\n",
    "                phase+'_classifier_acc': epoch_classifier_acc,\n",
    "            }\n",
    "\n",
    "\n",
    "            print('{} Loss: {:.4f} Accuracy: {:.4f}'.format(\n",
    "                phase, epoch_loss,epoch_acc))\n",
    "\n",
    "\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'test' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "\n",
    "                if torch.cuda.device_count() > 1:\n",
    "                    best_model_wts = copy.deepcopy(model.module.state_dict())\n",
    "                else:\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "        if epoch%2 == 0 :\n",
    "\n",
    "            torch.save(best_model_wts,os.path.join(os.getcwd(),'saved_models/'+str(args.tensorname)+'.t7'))\n",
    "\n",
    "\n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model,summary\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-house",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDeMeNet,summary = train_model(IDeMeNet, num_epochs=120)\n",
    "with open('summary.txt', 'w') as f:\n",
    "    print(summary, file=f)\n",
    "\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    torch.save(IDeMeNet.module.state_dict(),os.path.join(os.getcwd(),'saved_models/'+str(args.tensorname)+'.t7'))\n",
    "else:\n",
    "    torch.save(IDeMeNet.state_dict(),os.path.join(os.getcwd(),'saved_models/'+str(args.tensorname)+'.t7'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-cleaning",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
